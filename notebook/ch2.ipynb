{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import os\n",
    "import random\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nptyping import NDArray\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "colors = ['#de3838', '#007bc3', '#ffd12a']\n",
    "markers = ['o', 'x', ',']\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "cmap = sns.diverging_palette(255, 0, as_cmap=True)  # カラーパレットの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.9068026334400001\n",
      "-0.96059601\n",
      "--------------\n",
      "value of happy end: 1.0\n",
      "value of happy end: -1.0\n"
     ]
    }
   ],
   "source": [
    "LIMIT_GAME_COUNT = 5\n",
    "HAPPY_END_BORDER = 4\n",
    "MOVE_PROB = 0.9\n",
    "\n",
    "\n",
    "def V(s: str, gamma: float = 0.99) -> float:\n",
    "    V : float = R(s) + gamma * max_V_on_next_state(s, gamma)\n",
    "    return V\n",
    "\n",
    "\n",
    "def R(s: str) -> float:\n",
    "    if s == 'happy_end':\n",
    "        return 1\n",
    "    elif s == 'bad_end':\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def max_V_on_next_state(s: str, gamma: float) -> float:\n",
    "    \"\"\"全ての行動で価値Vを計算し、値が最大のVを返却\"\"\"\n",
    "    # If game end, expected value is 0.\n",
    "    if s in ['happy_end', 'bad_end']:\n",
    "        return 0\n",
    "    \n",
    "    actions : List[str] = ['up', 'down']\n",
    "    values : List[float] = []\n",
    "    for a in actions:\n",
    "        transition_probs : Dict[str, float] = transit_func(s, a)\n",
    "        v : float = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob : float = transition_probs[next_state]\n",
    "            v += prob * V(next_state, gamma)\n",
    "        values.append(v)\n",
    "    return max(values)\n",
    "\n",
    "\n",
    "def transit_func(s: str, a: str) -> Dict[str, float]:\n",
    "    \"\"\"Make next state by adding action str to state.\n",
    "\n",
    "    UnitTests\n",
    "    ---------\n",
    "    >>> transit_func(s='state', a='up')\n",
    "    {'state_up': MOVE_PROB, 'state_down': 1 - MOVE_PROB}\n",
    "    >>> transit_func(s='state_up_up_up_up_up', a='up')\n",
    "    {'happy_end': 1.0}\n",
    "    \"\"\"\n",
    "    actions : List[str] = s.split('_')[1:]\n",
    "\n",
    "    def next_state(state: str, action: str) -> str:\n",
    "        return '_'.join([state, action])\n",
    "\n",
    "    if len(actions) == LIMIT_GAME_COUNT:\n",
    "        up_count : int = sum([1 if a == 'up' else 0 for a in actions])\n",
    "        state : str = 'happy_end' if up_count >= HAPPY_END_BORDER else 'bad_end'\n",
    "        prob : float = 1.0\n",
    "        return {state: prob}\n",
    "    else:\n",
    "        opposite : str = 'up' if a == 'down' else 'down'\n",
    "        return {\n",
    "            next_state(s, a): MOVE_PROB,\n",
    "            next_state(s, opposite): 1 - MOVE_PROB}\n",
    "\n",
    "    \n",
    "\n",
    "print(V(s='state'))\n",
    "print(V(s='state_up_up'))\n",
    "print(V(s='state_down_down'))\n",
    "\n",
    "print('--------------')\n",
    "print(f'value of happy end: {V(s=\"happy_end\")}')\n",
    "print(f'value of happy end: {V(s=\"bad_end\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動的計画法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from ch1 import Environment, State, Action\n",
    "\n",
    "\n",
    "class Planner(ABCMeta):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        self.env : Environment = env\n",
    "        self.log : List[str] = []\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        self.env.reset()\n",
    "        self.log : List[List[float]] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def plan(self, gamma: float = 0.9, threshold: float = 0.0001):\n",
    "        raise NotImplementedError('Planner have to implements plan method.')\n",
    "\n",
    "    def transitions_at(self, state, action) -> Iterator[Tuple[float, State, float]]:\n",
    "        \"\"\"遷移確率T(s'|s,a)、次の状態s'、報酬を返却R(s,a,s')を返却\"\"\"\n",
    "        transition_probs : Dict[State, float] = self.env.transit_func(state=state, action=action)\n",
    "        for next_state in transition_probs:\n",
    "            prob : float = transition_probs[next_state]\n",
    "            reward, _ = self.env.reward_func(state=next_state)\n",
    "            yield prob, next_state, reward\n",
    "\n",
    "    def dict_to_grid(self, state_reward_dict: Dict[State, float]) -> List[List[float]]:\n",
    "        grid : List[List[float]] = []\n",
    "        for i in range(self.env.row_length):\n",
    "            row : List[int] = [0] * self.env.col_length\n",
    "            grid.append(row)\n",
    "        for s in state_reward_dict:\n",
    "            grid[s.row][s.col] = state_reward_dict[s]\n",
    "        \n",
    "        return grid    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationPlaner(Planner):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__(env)\n",
    "\n",
    "    def plan(self, gamma: float = 0.9, threshold: float = 0.0001) -> List[List[float]]:\n",
    "        \"\"\"動的計画法により各状態の状態価値関数V(s)を更新後、各々の状態を要素とする状態価値関数のListを返却\"\"\"\n",
    "        self.initialize()\n",
    "        actions : List[Action] = self.env.actions\n",
    "        V : Dict[State, float] = {s: 0 for s in self.env.states}  # Initialize each state's expected reward.\n",
    "\n",
    "        # 更新幅がthresholdを下回るまで、状態価値関数Vを更新\n",
    "        while True:\n",
    "            delta : float = 0\n",
    "            self.log.append(self.dict_to_grid(state_reward_dict=V))\n",
    "            for s in V:\n",
    "                if not self.env.can_action_at(state=s):\n",
    "                    continue\n",
    "                expected_rewards : List[float] = []  # ある状態sに関して、取り得る状態価値関数を保存\n",
    "                for a in actions:\n",
    "                    r : float = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(state=s, action=a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                max_reward : float = max(expected_rewards)  # アクションaに対する最大の状態価値関数: V(s) = max_a\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "            \n",
    "            if delta < threshold:\n",
    "                break\n",
    "        \n",
    "        V_grid : List[List[float]] = self.dict_to_grid(state_reward_dict=V)\n",
    "        return V_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationPlanner(Planner):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__(env)\n",
    "        self.policy = {}\n",
    "    \n",
    "    def initialize(self) -> None:\n",
    "        super().initialize()\n",
    "        self.policy : Dict[State, Dict[Action, float]] = {}\n",
    "        actions : List[Action] = self.env.actions\n",
    "        states : List[State] = self.env.states\n",
    "        for s in states:\n",
    "            self.policy[s] = {}\n",
    "            for a in actions:\n",
    "                # Initialize policy.\n",
    "                # At first, each action is taken uniformly.\n",
    "                self.policy[s][a] = 1 / len(actions)\n",
    "\n",
    "    def estimate_by_policy(self, gamma: float, threshold: float) -> Dict[State, float]:\n",
    "        \"\"\"方策評価\"\"\"\n",
    "        V : Dict[State, float] = {s: 0 for s in self.env.states}  # Initialize each state's expected reward.\n",
    "\n",
    "        while True:\n",
    "            delta : float = 0\n",
    "            for s in V:\n",
    "                expected_rewards : List[float] = []\n",
    "                for a in self.policy[s]:\n",
    "                    action_prob : float = self.policy[s][a]\n",
    "                    r : float = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(state=s, action=a):\n",
    "                        r += action_prob * prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                value : float = sum(expected_rewards)\n",
    "                delta = max(delta, abs(value - V[s]))\n",
    "                V[s] = value\n",
    "            if delta < threshold:\n",
    "                break\n",
    "        \n",
    "        return V\n",
    "\n",
    "    def plan(self, gamma: float = 0.9, threshold: float = 0.0001) -> List[List[float]]:\n",
    "        \"\"\"動的計画法により各状態の状態価値関数V(s)を更新後、各々の状態を要素とする状態価値関数のListを返却\"\"\"\n",
    "        self.initialize()\n",
    "        states : List[State] = self.env.states\n",
    "        actions : List[Action] = self.env.actions\n",
    "        \n",
    "        def take_max_action(action_value_dict: Dict[Action, float]) -> Action:\n",
    "            \"\"\"valueが最大となるkeyを取得\n",
    "            \n",
    "            UnitTests\n",
    "            ---------\n",
    "            >>> take_max_action(action_value_dict={'UP': 0.5, 'DOWN': 0.7})\n",
    "            'DOWN'\n",
    "            \"\"\"\n",
    "            return max(action_value_dict, key=action_value_dict.get)\n",
    "\n",
    "        # 更新幅がthresholdを下回るまで、状態価値関数Vを更新\n",
    "        while True:\n",
    "            update_stable : bool = True\n",
    "            # Estimate expected rewards under current policy.\n",
    "            V : Dict[State, float] = self.estimate_by_policy(gamma=gamma, threshold=threshold)  # 方策評価により状態価値関数V(s)を改善\n",
    "            self.log.append(self.dict_to_grid(state_reward_dict=V))\n",
    "\n",
    "            for s in states:\n",
    "                # Get an action following to the current policy.\n",
    "                policy_action : Action = take_max_action(action_value_dict=self.policy[s])  # ポリシー確率π(a, s)によって採択されたaction\n",
    "\n",
    "                # Compare with other actions.\n",
    "                action_rewards : Dict[Action, float] = {}\n",
    "                for a in actions:\n",
    "                    r : float = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(state=s, action=a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    action_rewards[a] = r\n",
    "                best_action : Action = take_max_action(action_value_dict=action_rewards)  # 状態価値関数V(s)を最大とするaction\n",
    "                if policy_action != best_action:\n",
    "                    update_stable = False\n",
    "                \n",
    "                # Update policy (set best_action prob=1, otherwise=0 (greedy))\n",
    "                for a in self.policy[s]:\n",
    "                    prob = 1 if a == best_action else 0\n",
    "                    self.policy[s][a] = prob\n",
    "\n",
    "            if update_stable:\n",
    "                # If policy isn't updated, stop iteration\n",
    "                break\n",
    "        \n",
    "        # Turn dictionary to grid\n",
    "        V_grid : List[List[float]] = self.dict_to_grid(state_reward_dict=V)\n",
    "        return V_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50bb28d91d8964d6677cc2a0866425352867b335ef92d97f6303b8ea15a1963f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
